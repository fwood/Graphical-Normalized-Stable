Hierarchical models for regularizing conditional density estimates are central to most applications in statistics and machine learning.  The sharing of statistical strength by tying together distributions is imperative for inference in models with a large number of parameters, especially when the number of parameters is larger than the number of observations.  

In this work we are motivated by the goal of maximizing sharing of statistical strength via learning of back-off paths in a dense, hierarchical graphical models.  One intuition for this comes from natural language modeling applications.  Consider the distribution of words that follow ``She ate 2 pieces of  $\ldots$''  In this case we will call ``she ate 2 pieces of'' the ``context.''  Recognize that the context indexes a conditional distribution (over words).  Traditional \ngram language modeling approaches to estimating the conditional distribution over words that follow this context would look for the string ``she ate 2 pieces of'' in the corpus and form a counting estimate of the conditional distribution from that.  Of course, this estimate will be extremely noisy in long contexts, and, more generally, any time the underlying stochastic process produces observations in a large vocabulary or has long-range dependencies.   So, what is usually done is to hierarchically relate the distribution over words that follow the context ``She ate 2 pieces of $\ldots$'' to the distribution over words that follow ``ate 2 pieces of $\ldots$'' and that to ``2 pieces of $\ldots$'' and so on.  Of course, in this case, it is clear that we would rather generalize via a different path.  In fact, we'd much rather generalize via the path ``She ate 2 pieces of $\ldots$'' is more specific than ``She ate * pieces of $\ldots$'' is more specific than ``* ate * pieces of $\ldots$'' is more specific than ``* ate * * of \ldots'' is more specific than ``* ate * * * \ldots.''  Of course, it goes without saying, each and every one of these contexts generalizes in many, many different ways.  More generally one can think about the context as a bag of features and the generalization paths accessible from that bag as functions that map specific contexts to less specific contexts in which one more more of the values of the features is disregarded.

Another example comes from modeling image statistics.  Consider filling in the central pixel in a $5\times5$ binary image patch.  Here we would like to estimate the conditional distribution of the pixel given its surround, however, finding patches that exactly match the surround is extremely unlikely even in highly regular domains.  Also, very clearly here, more back-off paths than just ignoring the value of a single pixel are essential.

Note that hidden or latent variables can be interpreted and utilized as features in this set up.  